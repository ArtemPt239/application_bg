{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d4f07-a127-4c8f-8c77-ebb423c33ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "from matplotlib.image import imread\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms.functional as fn\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "from torch import nn, Tensor\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a42bb-83e3-4726-8b33-90acec5d1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/local/tmp/BostonGene_application_data/images/\"\n",
    "processed_image_size = 64\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e396adf-89ca-4379-bb0d-1557b598cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = os.listdir(dataset_dir)\n",
    "# _filenames = {target: [file for file in os.listdir(f\"{dataset_dir}/{target}\")] for target in targets}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by writing a wrapper class for our dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb74d8a-b019-48b3-a78d-d31822c72682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir: str, image_size: int = 64, train: bool = True, manual_ids: tp.Optional[bool] = None,\n",
    "                 augmentations=None):\n",
    "        \"\"\"\n",
    "        A wrapper class for our dataset.\n",
    "\n",
    "        Since our dataset is small, we simply store it in RAM.\n",
    "\n",
    "        :param dataset_dir: location of the upacked dataset\n",
    "        :param image_size: size, to which the image will be resized during preprocessing of the image\n",
    "        :param train: set True for train, or False for test\n",
    "        :param manual_ids: override default train\\test ids split by manually choosing which ids to load. Ids should be chosen from MyDataset.get_ids()\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._train = train\n",
    "        self.target_names = os.listdir(dataset_dir)\n",
    "        self.target_encoding = {self.target_names[_id]: _id for _id in range(len(self.target_names))}\n",
    "\n",
    "        targets_and_filenames = [(target, filename) for target in self.target_names for filename in\n",
    "                                 os.listdir(f\"{dataset_dir}/{target}\")]\n",
    "\n",
    "        if manual_ids is None:\n",
    "            targets = [self.target_encoding[item[0]] for item in targets_and_filenames]\n",
    "            train_ids, test_ids = train_test_split(list(range(len(targets_and_filenames))), random_state=42,\n",
    "                                                   test_size=0.1, shuffle=True, stratify=targets)\n",
    "            ids = train_ids if self._train else test_ids\n",
    "        else:\n",
    "            ids = manual_ids\n",
    "\n",
    "        # Implementing augmentations\n",
    "        if self._train and augmentations is not None:\n",
    "            self.is_augmented = True\n",
    "            preprocessing_pipeline = transforms.Compose([\n",
    "                transforms.Lambda(lambda img: img / 255),  # Normalize\n",
    "            ])\n",
    "            self.augmentation_pipeline = transforms.Compose([\n",
    "                augmentations,\n",
    "                transforms.Lambda(lambda img: transforms.RandomCrop(size=min(img.shape[1:]))(img)),\n",
    "                # Randomly crop to square\n",
    "                transforms.Resize(size=image_size)\n",
    "            ])\n",
    "        else:\n",
    "            self.is_augmented = False\n",
    "            preprocessing_pipeline = transforms.Compose([\n",
    "                transforms.Lambda(lambda img: img / 255),  # Normalize\n",
    "                transforms.Lambda(lambda img: transforms.CenterCrop(size=min(img.shape[1:]))(img)),  # Crop to square\n",
    "                transforms.Resize(size=image_size)\n",
    "            ])\n",
    "\n",
    "        self._data = []\n",
    "        self._targets = []\n",
    "        for target, file in np.array(targets_and_filenames)[ids]:\n",
    "            self._data.append(preprocessing_pipeline(\n",
    "                torch.tensor(imread(f\"{dataset_dir}/{target}/{file}\"), dtype=torch.float32).permute(2, 1, 0)))\n",
    "            self._targets.append(self.target_encoding[target])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._targets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = self.augmentation_pipeline(self._data[item]) if self.is_augmented else self._data[item]\n",
    "        return img, self._targets[item]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ids(dataset_dir: str):\n",
    "        return list(range(sum([len(os.listdir(f\"{dataset_dir}/{target}\")) for target in os.listdir(f\"{dataset_dir}\")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(dataset_dir, processed_image_size, train=True,\n",
    "                          augmentations=transforms.Compose([\n",
    "                              transforms.RandomRotation(10),\n",
    "                              transforms.RandomHorizontalFlip(p=0.1),\n",
    "                              transforms.RandomVerticalFlip(p=0.1)\n",
    "                          ])\n",
    "                          )\n",
    "test_dataset = MyDataset(dataset_dir, processed_image_size, train=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6873a-ba8d-431b-9cc2-00ec8996170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 1\n",
    "print(\"Before:\")\n",
    "plt.imshow(train_dataset._data[img_id].permute(2, 1, 0))\n",
    "plt.show()\n",
    "print(\"After:\")\n",
    "plt.imshow(train_dataset[img_id][0].permute(2, 1, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a130943-6475-4bdf-ae37-bc9c84384125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210641c-1b69-47df-b52b-06a565e6151c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621033d-78ec-4e46-a6b1-347a79317277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple baseline - a ResNet model\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "        self.downsample = False\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = True\n",
    "            self.downsample_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                              kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, _input: Tensor) -> Tensor:\n",
    "        identity = _input\n",
    "        if self.downsample:\n",
    "            identity = self.downsample_layer(identity)\n",
    "\n",
    "        output = self.conv1(_input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output += identity\n",
    "        output = self.activation(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class MyResNet(nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self, input_channels: int, layers: list[tuple[int, int]], num_of_classes: int, initial_layers:\n",
    "    tp.Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if initial_layers is None:\n",
    "            initial_layers = nn.Identity()\n",
    "        self.initial_layers = initial_layers\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "        for layer_id, layer in enumerate(layers):\n",
    "            num_of_blocks, channels_per_block = layer\n",
    "            for block_id in range(num_of_blocks):\n",
    "                if block_id == 0:\n",
    "                    if layer_id == 0:\n",
    "                        in_channels = input_channels\n",
    "                    else:\n",
    "                        in_channels = layers[layer_id - 1][1]\n",
    "                else:\n",
    "                    in_channels = channels_per_block\n",
    "                self.layers.add_module(f\"ResBlock{layer_id}_{block_id}\",\n",
    "                                       ResBlock(in_channels=in_channels, out_channels=channels_per_block))\n",
    "\n",
    "            if layer_id != len(layers) - 1:\n",
    "                self.layers.add_module(f\"pool{layer_id}\", nn.MaxPool2d(2))\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc = nn.Linear(in_features=layers[-1][1], out_features=num_of_classes)\n",
    "\n",
    "    def forward(self, _input: Tensor) -> Tensor:\n",
    "        output = self.initial_layers(_input)\n",
    "        output = self.layers(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = torch.flatten(output, 1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Напишем стандартный код, который можно использовать для обучения моделек:\n",
    "\n",
    "def trainer(number_of_epochs,\n",
    "            dataset,\n",
    "            val_dataset,\n",
    "            batch_size,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            writer,\n",
    "            lr=0.001,\n",
    "            lr_multiplier_schedule=None):\n",
    "    def make_val_report(iteration):\n",
    "        report = calculate_val_performance(model, DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8))\n",
    "\n",
    "        def report_avg(which_avg):\n",
    "            for metric in report[which_avg]:\n",
    "                if metric != 'support':\n",
    "                    writer.add_scalar(f'metrics/{which_avg}/{metric}', report[which_avg][metric], iteration)\n",
    "\n",
    "        report_avg('weighted avg')\n",
    "        report_avg('macro avg')\n",
    "        writer.add_scalar(f'metrics/accuracy', report['accuracy'], iteration)\n",
    "\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "    iterations = tqdm(range(number_of_epochs), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "    for it in iterations:\n",
    "        if lr_multiplier_schedule is not None and it in lr_multiplier_schedule:\n",
    "            lr *= lr_multiplier_schedule[it]\n",
    "            optima = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "        epoch_loss = train_epoch(train_generator=DataLoader(dataset, batch_size, shuffle=True, num_workers=8),\n",
    "                                 model=model,\n",
    "                                 loss_function=loss_function,\n",
    "                                 optimizer=optima)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n",
    "        writer.add_scalar('metrics/train_loss', epoch_loss, it)\n",
    "        make_val_report(it)\n",
    "\n",
    "\n",
    "def train_epoch(train_generator, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for x, y in train_generator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x.to(model.device))\n",
    "\n",
    "        loss = loss_function(output, y.to(model.device))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.cpu().item()\n",
    "        total += 1\n",
    "\n",
    "    return epoch_loss / total\n",
    "\n",
    "\n",
    "def calculate_val_performance(model, val_dataset):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = [int(torch.argmax(model(x.to(model.device)).cpu())) for x, y in val_dataset]\n",
    "    y_target = [int(y) for x, y in val_dataset]\n",
    "\n",
    "    return sklearn.metrics.classification_report(y_target, y_pred, output_dict=True, zero_division=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5b076-4d49-4f14-9f26-3c6c7942f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyResNet(input_channels=3, layers=[(4, 64), (4, 128)], num_of_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "trainer(number_of_epochs=10,\n",
    "        dataset=train_dataset,\n",
    "        val_dataset=test_dataset,\n",
    "        batch_size=16,\n",
    "        model=model,\n",
    "        loss_function=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.Adam,\n",
    "        writer=SummaryWriter(log_dir='/local/tmp/logs/BG_application/resnet'),\n",
    "        lr=0.001,\n",
    "        lr_multiplier_schedule={1: 0.5, 3: 0.5, 5: 0.5, 7: 0.5})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0.42-0.44"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
